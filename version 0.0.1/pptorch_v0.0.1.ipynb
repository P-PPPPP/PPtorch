{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import pandas as pd;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    # 类初始化\n",
    "    def __init__(self):\n",
    "        self.model_state = {'compile_model':False,'loading_data':False};\n",
    "        self.structure = {};\n",
    "        self.num_layers = 0;\n",
    "        self.parameter = {};\n",
    "        self.info = {};\n",
    "        self.layers = self.class_layers();\n",
    "        self.res_layer ={};\n",
    "        self.median_value = {};\n",
    "        self.error = None;\n",
    "        self.nets = [];\n",
    "        self.in_shape = [];\n",
    "        self.out_shape = [];\n",
    "    # 定义层\n",
    "    class class_layers:    \n",
    "        def __init__(self):\n",
    "            pass;\n",
    "        def flatten(self,input_shape = None):\n",
    "            # 该方法将任意输入形状数据展平;\n",
    "            return {'type':'flatten','input_shape':input_shape};\n",
    "        def dense(self,num_nets = None,activation = 'relu',learning_rate=0.01): \n",
    "            # 一层全连接神经网络\n",
    "            return {'type':'dense','num_nets':num_nets,'activation_func':activation,'learning_rate':learning_rate};\n",
    "    # 随机打乱输入数据，将数据划分为训练/测试集\n",
    "    def random_division(self,x,y=None,test_rate = 0):\n",
    "        x = np.array(x);\n",
    "        # 数据总长度\n",
    "        n = np.shape(x)[0];\n",
    "        # 生成下表序列并随即打乱\n",
    "        sequence = np.array(range(n));\n",
    "        np.random.shuffle(sequence);\n",
    "        # 测试集长度\n",
    "        n_test = int(np.ceil(n * test_rate));\n",
    "        # 测试集序列与训练集序列\n",
    "        se_test = sequence[:n_test];\n",
    "        se_train = sequence[n_test:];\n",
    "        # 训练集\n",
    "        train_data = x[se_train];\n",
    "        # 测试集\n",
    "        test_data = x[se_test];\n",
    "        # 有监督学习的情况\n",
    "        if not type(y) == type(None):\n",
    "            y = np.array(y);\n",
    "            train_label = y[se_train];\n",
    "            test_label = y[se_test];\n",
    "            if not test_rate == 0:\n",
    "                return (train_data,train_label,test_data,test_label);\n",
    "            else:\n",
    "                return (train_data,train_label);\n",
    "        # 无监督情况\n",
    "        if not test_rate == 0:\n",
    "            return (train_data,test_data);\n",
    "        else:\n",
    "            return (train_data);\n",
    "    # 归一化\n",
    "    def normalization(self,x_train,x_test=None,norm_type= 'max_abs'):\n",
    "        # 最大绝对值放缩\n",
    "        if norm_type == 'max_abs':\n",
    "            max_abs = np.max(np.abs(x_train),axis=0)\n",
    "            x_train = x_train/max_abs;\n",
    "            if not type(x_test) == type(None):\n",
    "                x_test = x_test / max_abs;\n",
    "        # 最大最小值归一化\n",
    "        if norm_type == 'max_min':\n",
    "            max_v = np.max(x_train);\n",
    "            min_v = np.min(x_train);\n",
    "            x_train = (max_v - x_train) / (max_v - min_v);\n",
    "            if not type(x_test) == type(None):\n",
    "                x_test = (max_v - x_test) / (max_v - min_v);\n",
    "        # 标准化\n",
    "        if norm_type == 'norm':\n",
    "            mean_v = np.mean(x_train,axis=0);\n",
    "            std_v = np.std(x_train,axis=0);\n",
    "            x_train = (max_v - mean_v) / std_v;\n",
    "            if not type(x_test) == type(None):\n",
    "                x_test = (max_v - mean_v) / std_v;\n",
    "        # 函数返回\n",
    "        if not type(x_test) == type(None):\n",
    "            return (x_train,x_test);\n",
    "        return x_train;\n",
    "    # 将数字标签转化为one hot标签\n",
    "    def onehot_label(self,label):\n",
    "        max_num = np.max(label);\n",
    "        res = [];\n",
    "        for k in range(len(label)):\n",
    "            resk = np.zeros((max_num+1,));\n",
    "            resk[label[k]] = 1;\n",
    "            res.append(resk);\n",
    "        return np.array(res);\n",
    "    # 向模型加载数据\n",
    "    def load_data(self,\n",
    "    train_data=None,\n",
    "    train_label=None,\n",
    "    validation_data=None,\n",
    "    validation_label=None,\n",
    "    random_train_set=True,\n",
    "    auto_test_set=False,\n",
    "    test_proportion=0.1,\n",
    "    data_normalization=False,\n",
    "    normalization_type='max_abs'\n",
    "    ):\n",
    "        # 自动划分测试集合\n",
    "        if auto_test_set == True:\n",
    "            if not type(train_label) == type(None):\n",
    "                # 有监督情况\n",
    "                (train_data,train_label,validation_data,validation_label) = self.random_division(train_data,train_label,test_rate=test_proportion);\n",
    "            else:\n",
    "                # 无监督情况\n",
    "                (train_data,validation_data) = self.random_division(train_data,test_rate=test_proportion);\n",
    "        # 训练数据随机化\n",
    "        elif random_train_set == True:\n",
    "            if not type(train_label) == type(None):\n",
    "                # 有监督情况\n",
    "                (train_data,train_label) = self.random_division(train_data,train_label,test_rate=0);\n",
    "            else:\n",
    "                # 无监督情况\n",
    "                (train_data) = self.random_division(train_data,test_rate=0);\n",
    "        # 数据归一化\n",
    "        if data_normalization == True:\n",
    "            if not type(validation_data) == type(None):\n",
    "                (train_data,validation_data) = self.normalization(train_data,validation_data,normalization_type);\n",
    "            else:\n",
    "                train_data = self.normalization(train_data,None,normalization_type);\n",
    "        # 记录加载训练集信息\n",
    "        if not type(train_data) == type(None):\n",
    "            self.train_data = train_data;\n",
    "            self.model_state.update({'loading_train_data':True});\n",
    "            self.info.update({'num_train_data':train_data.shape[0]});\n",
    "            self.n_train = train_data.shape[0];\n",
    "        if not type(train_label) == type(None):\n",
    "            self.train_label = train_label;\n",
    "            self.model_state.update({'loading_train_label':True});\n",
    "            self.d_train = train_data.shape[1:];\n",
    "        # 记录加载验证集信息\n",
    "        if not type(validation_data) == type(None):\n",
    "            self.validation_data = validation_data;\n",
    "            self.model_state.update({'loading_validation_data':True});\n",
    "            self.info.update({'num_validation_data':validation_data.shape[0]});\n",
    "            self.n_val = validation_data[0];\n",
    "        if not type(validation_label) == type(None):\n",
    "            self.validation_label = validation_label;\n",
    "            self.model_state.update({'loading_validation_label':True});\n",
    "            self.d_val = validation_data[1:];\n",
    "        # 记录数据加载信息\n",
    "        if self.model_state['loading_train_data'] == True and self.model_state['loading_train_label'] == True:\n",
    "            self.model_state['loading_data'] = True;\n",
    "    # 描述新的网络结构\n",
    "    def define(self,BP_structure = None):\n",
    "        if not type(BP_structure) == type(None):\n",
    "            self.layer_type = [];\n",
    "            for layer in BP_structure:\n",
    "                self.structure.update({self.num_layers:layer});\n",
    "                self.layer_type.append(layer['type']);\n",
    "                self.num_layers += 1;\n",
    "    # 向已有结构新添层\n",
    "    def add(self,new_layer):\n",
    "        self.network_structure.update({self.num_layers:new_layer});\n",
    "        self.num_layers += 1;\n",
    "    # 网络编译初始化参数\n",
    "    def compile_init(self):\n",
    "        self.x = [];\n",
    "        self.y = [];\n",
    "        self.w = [];\n",
    "        self.b =[];\n",
    "        self.nets = [];\n",
    "        self.rate = [];\n",
    "        self.act_func =[];\n",
    "        self.delta = [];\n",
    "    # 编译网络输入输出的维度信息\n",
    "    def compile_dim_info(self):\n",
    "        n = self.n_train;\n",
    "        d = self.d_train;\n",
    "        out_shape = d;\n",
    "        for k in range(self.num_layers):\n",
    "            layer = self.structure[k];\n",
    "            if layer['type'] == 'flatten':\n",
    "                in_shape = out_shape;\n",
    "                out_shape = 1;\n",
    "                for p in d:\n",
    "                    out_shape *= p;\n",
    "                self.nets.append(np.array([]));\n",
    "                self.in_shape.append(in_shape);\n",
    "                self.out_shape.append(out_shape);\n",
    "                continue;\n",
    "            if layer['type'] == 'dense':\n",
    "                in_shape = out_shape;\n",
    "                out_shape = layer['num_nets'];\n",
    "                self.nets.append(layer['num_nets']);\n",
    "                self.in_shape.append(in_shape);\n",
    "                self.out_shape.append(out_shape);\n",
    "                continue;\n",
    "    # 编译网络所需要的中间量\n",
    "    def compile_param_info(self):\n",
    "        for k in range(self.num_layers):\n",
    "            layer = self.structure[k];\n",
    "            if layer['type'] == 'flatten':\n",
    "                self.x.append(np.array([]));\n",
    "                self.y.append(np.array([]));\n",
    "                self.w.append(np.array([]));\n",
    "                self.b.append(np.array([]));\n",
    "                self.rate.append(np.array([]));\n",
    "                self.act_func.append(np.array([]));\n",
    "                self.delta.append(np.array([]));\n",
    "                continue;\n",
    "            if layer['type'] == 'dense':\n",
    "                N1 = self.in_shape[k];\n",
    "                N2 = self.out_shape[k];\n",
    "                w = np.random.randn(N1,N2);\n",
    "                b = np.random.randn(1,N2);\n",
    "                self.x.append(np.array([]));\n",
    "                self.y.append(np.array([]));\n",
    "                self.w.append(w);\n",
    "                self.b.append(b);\n",
    "                self.rate.append(layer['learning_rate']);\n",
    "                self.act_func.append(layer['activation_func']);\n",
    "                self.delta.append(np.array([]));\n",
    "                continue;\n",
    "\n",
    "        if not N2 == len(self.train_label[0]):\n",
    "            raise Exception(\"UNFITTED OUTPUT LAYER\");\n",
    "    # 编译该神经网络\n",
    "    def compile(self,loss_function = 'MSE',optimizer='SGD',metrics = None):\n",
    "        self.info.update({'loss_function':loss_function,'optimizer':optimizer,'metrics':metrics});\n",
    "        self.compile_init();\n",
    "        # 维度信息\n",
    "        self.compile_dim_info();\n",
    "        # 参数信息\n",
    "        self.compile_param_info();\n",
    "        self.model_state['compile_model']=True;\n",
    "    # 模型训练\n",
    "    def fit(self,epoches=1,batch = -1):\n",
    "        # 防止未加载数据或未编译网络时训练\n",
    "        if self.model_state['compile_model'] == False or self.model_state['loading_data'] == False:\n",
    "            raise Exception(\"UNINITIALIZED MODEL: (loading data:{}, compile model:{})\".format(\n",
    "                self.model_state['loading_data'],self.model_state['compile_model']));\n",
    "        # BGD优化训练\n",
    "        if self.info['optimizer'] == 'BGD':\n",
    "            self.BGD(epoches,batch);\n",
    "    # 前向传播\n",
    "    def forward_propagation(self,x0):\n",
    "        # 输入层\n",
    "        if self.layer_type[0] == 'flatten':\n",
    "            x = x0;\n",
    "            y = [];\n",
    "            for i in range(x.shape[0]):\n",
    "                xk = x[i];\n",
    "                y.append(xk.reshape(-1));\n",
    "        elif self.layer_type[0] == 'dense':\n",
    "            x = np.dot(x0,self.w[0]) + self.b[0];\n",
    "            y = self.func(self.act_func[0],x);\n",
    "        self.x[0] = x;\n",
    "        self.y[0] = np.array(y);\n",
    "        # 隐藏层\n",
    "        for k in range(1,len(self.nets)):\n",
    "            if self.layer_type[k] == 'flatten':\n",
    "                x = self.y[k-1];\n",
    "                y = [];\n",
    "                for i in range(len(x)):\n",
    "                    xk = x[i];\n",
    "                    y.append(xk.reshape(-1));\n",
    "                self.x[k] = x;\n",
    "                self.y[k] = np.array(y);\n",
    "            elif self.layer_type[k] == 'dense':\n",
    "                x = np.dot(self.y[k-1],self.w[k]) + self.b[k];\n",
    "                y = self.func(self.act_func[k],x);\n",
    "                self.x[k] = x;\n",
    "                self.y[k] = y;\n",
    "    # 反向传播\n",
    "    def backward_propagation(self,label):\n",
    "        # 输出层情况\n",
    "        k = len(self.nets) - 1;\n",
    "        delta = self.gra_func(self.info['loss_function'],self.y[k],label) * self.gra_func(self.act_func[k],self.x[k]);\n",
    "        db = np.sum(delta,axis=0).reshape(1,-1);\n",
    "        dw = np.dot(self.y[k-1].T,delta);\n",
    "        self.b[k] -= self.rate[k]*db;\n",
    "        self.w[k] -= self.rate[k]*dw;\n",
    "        self.delta[k] = delta;\n",
    "        k -= 1;\n",
    "        # 隐藏层\n",
    "        while(k>0):\n",
    "            if self.layer_type[k] == 'flatten':\n",
    "                delta = self.delta[k+1];\n",
    "                delta = delta.reshape(-1,self.in_shape[k]);\n",
    "                self.delta[k] = delta;\n",
    "            elif self.layer_type[k] == 'dense':\n",
    "                delta = np.dot(self.delta[k+1],self.w[k+1].T) * self.gra_func(self.act_func[k],self.x[k]);\n",
    "                db = np.sum(delta,axis = 0).reshape(1,-1);\n",
    "                dw = np.dot(self.y[k-1].T,delta);\n",
    "                self.b[k] -= self.rate[k]*db;\n",
    "                self.w[k] -= self.rate[k]*dw;\n",
    "                self.delta[k] = delta;\n",
    "            k -= 1;\n",
    "    # activation and loss function \n",
    "    def func(self,f,x,y=None):\n",
    "        if f == 'relu':\n",
    "            res = np.abs(x);\n",
    "            res = (res + x)/2;\n",
    "            return res;\n",
    "        if f == 'x':\n",
    "            return x;\n",
    "        # loss function\n",
    "        if f == 'MSE':\n",
    "            res = (x - y)**2\n",
    "            res = 1/x.shape[0] * 1/2 * (np.sum(res));# 按样本数量归一化\n",
    "            return res;\n",
    "    # 导数\n",
    "    def gra_func(self,f,x,y=None):\n",
    "        # activation function\n",
    "        if f == 'relu':\n",
    "            res = np.abs(x);\n",
    "            res = np.sign(res + x);\n",
    "            return res;\n",
    "        if f == 'x':\n",
    "            return 1;\n",
    "        # loss function\n",
    "        if f == 'MSE':\n",
    "            res = (x - y)\n",
    "            return res;\n",
    "    # 在定义batch时，将原数据分段\n",
    "    def batch_slice(self,batch,n):\n",
    "        train_list = [];\n",
    "        if batch <= 0 or batch >n:\n",
    "            batch = n;\n",
    "            train_list.append([0,n]);\n",
    "        else:\n",
    "            m = n//batch;\n",
    "            for k in range(m):\n",
    "                train_list.append([k*batch,(k+1)*batch]);\n",
    "            if not n%batch == 0:\n",
    "                train_list.append([(k+1)*batch,n]);\n",
    "        return train_list;\n",
    "    # BGD优化器\n",
    "    def BGD(self,epoches,batch = -1):\n",
    "        if batch <= 0 or batch > self.n_train:\n",
    "            batch = self.n_train;\n",
    "        trian_list = self.batch_slice(batch,self.n_train)\n",
    "        for p in range(epoches):\n",
    "            self.timer = 0;\n",
    "            self.state_display(state=1,epoch=p,epoches=epoches);\n",
    "            for k in trian_list:\n",
    "                self.timer += 1;\n",
    "                x0 = self.train_data[k[0]:k[1]];\n",
    "                label = self.train_label[k[0]:k[1]];\n",
    "                \n",
    "                # 正向传播\n",
    "                self.forward_propagation(x0);\n",
    "                # 计算误差\n",
    "                loss = self.func(self.info['loss_function'],self.y[-1],label);\n",
    "                # 反向传播\n",
    "                self.backward_propagation(label);\n",
    "                # 输出误差\n",
    "                self.state_display(state=2,loss=loss,all=len(trian_list));\n",
    "            print('');\n",
    "    # 训练状态信息输出\n",
    "    def state_display(self,state,loss=-1,epoch=1,epoches=1,batch=-1,all=-1):\n",
    "        # title\n",
    "        if state == 1:\n",
    "            print('Epoch {}/{}'.format(epoch+1,epoches));\n",
    "            self.n_str = 0;\n",
    "        # loading bar\n",
    "        if state == 2:\n",
    "            self.loading_bar(self.timer/all,all,loss);\n",
    "    # 进度条\n",
    "    def loading_bar(self,process,all,loss):\n",
    "        n = int(process*30) + 1;\n",
    "        m = 30-n;\n",
    "        s = '{}/{} '.format(self.timer,all);\n",
    "        s += '[' + '='*n + '-'*m + ']';\n",
    "        s += '\\tloss: {:.2f}'.format(loss);\n",
    "        n = len(s);\n",
    "        s += ' '*(self.n_str - n);\n",
    "        print('\\r',end='')\n",
    "        print(s,end='');\n",
    "        if n > self.n_str:\n",
    "            self.n_str = n;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "names_to_label = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2};\n",
    "label_to_names = {value: key for key, value in names_to_label.items()}\n",
    "df = pd.read_csv('iris.data', header=None)\n",
    "xs = df.iloc[:, :4].values\n",
    "ts = np.array([names_to_label[name] for name in df.iloc[:, -1]])\n",
    "ts = ts.reshape(-1,1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调取模组\n",
    "pp = model();\n",
    "# 加载数据\n",
    "pp.load_data(\n",
    "    train_data = xs,\n",
    "    train_label = ts,\n",
    "    auto_test_set=True,# 从训练集划分出测试集\n",
    "    test_proportion=0.1,# 划分的测试集的比例\n",
    "    data_normalization=True,# 对数据归一化\n",
    "    normalization_type='max_abs'# 归一化的类型\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络\n",
    "pp.define([\n",
    "    pp.layers.flatten(),\n",
    "    pp.layers.dense(num_nets=32,activation='relu',learning_rate=1e-8),\n",
    "    pp.layers.dense(num_nets=128,activation='relu',learning_rate=1e-8),\n",
    "    pp.layers.dense(num_nets=1,learning_rate=1e-8)\n",
    "])\n",
    "# 编译网络\n",
    "pp.compile(loss_function = 'MSE',optimizer='BGD');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [===============================]\tloss: 89007.32\n",
      "Epoch 2/100\n",
      "1/1 [===============================]\tloss: 74855.59\n",
      "Epoch 3/100\n",
      "1/1 [===============================]\tloss: 62977.92\n",
      "Epoch 4/100\n",
      "1/1 [===============================]\tloss: 53030.44\n",
      "Epoch 5/100\n",
      "1/1 [===============================]\tloss: 44706.37\n",
      "Epoch 6/100\n",
      "1/1 [===============================]\tloss: 37737.46\n",
      "Epoch 7/100\n",
      "1/1 [===============================]\tloss: 31902.87\n",
      "Epoch 8/100\n",
      "1/1 [===============================]\tloss: 27018.10\n",
      "Epoch 9/100\n",
      "1/1 [===============================]\tloss: 22920.69\n",
      "Epoch 10/100\n",
      "1/1 [===============================]\tloss: 19487.39\n",
      "Epoch 11/100\n",
      "1/1 [===============================]\tloss: 16608.16\n",
      "Epoch 12/100\n",
      "1/1 [===============================]\tloss: 14194.39\n",
      "Epoch 13/100\n",
      "1/1 [===============================]\tloss: 12171.44\n",
      "Epoch 14/100\n",
      "1/1 [===============================]\tloss: 10477.09\n",
      "Epoch 15/100\n",
      "1/1 [===============================]\tloss: 9056.04\n",
      "Epoch 16/100\n",
      "1/1 [===============================]\tloss: 7862.56\n",
      "Epoch 17/100\n",
      "1/1 [===============================]\tloss: 6857.77\n",
      "Epoch 18/100\n",
      "1/1 [===============================]\tloss: 6013.11\n",
      "Epoch 19/100\n",
      "1/1 [===============================]\tloss: 5301.08\n",
      "Epoch 20/100\n",
      "1/1 [===============================]\tloss: 4696.80\n",
      "Epoch 21/100\n",
      "1/1 [===============================]\tloss: 4182.17\n",
      "Epoch 22/100\n",
      "1/1 [===============================]\tloss: 3743.40\n",
      "Epoch 23/100\n",
      "1/1 [===============================]\tloss: 3368.14\n",
      "Epoch 24/100\n",
      "1/1 [===============================]\tloss: 3045.47\n",
      "Epoch 25/100\n",
      "1/1 [===============================]\tloss: 2766.35\n",
      "Epoch 26/100\n",
      "1/1 [===============================]\tloss: 2524.63\n",
      "Epoch 27/100\n",
      "1/1 [===============================]\tloss: 2314.72\n",
      "Epoch 28/100\n",
      "1/1 [===============================]\tloss: 2131.97\n",
      "Epoch 29/100\n",
      "1/1 [===============================]\tloss: 1972.74\n",
      "Epoch 30/100\n",
      "1/1 [===============================]\tloss: 1833.73\n",
      "Epoch 31/100\n",
      "1/1 [===============================]\tloss: 1711.90\n",
      "Epoch 32/100\n",
      "1/1 [===============================]\tloss: 1604.96\n",
      "Epoch 33/100\n",
      "1/1 [===============================]\tloss: 1510.72\n",
      "Epoch 34/100\n",
      "1/1 [===============================]\tloss: 1426.99\n",
      "Epoch 35/100\n",
      "1/1 [===============================]\tloss: 1351.90\n",
      "Epoch 36/100\n",
      "1/1 [===============================]\tloss: 1284.26\n",
      "Epoch 37/100\n",
      "1/1 [===============================]\tloss: 1223.30\n",
      "Epoch 38/100\n",
      "1/1 [===============================]\tloss: 1168.30\n",
      "Epoch 39/100\n",
      "1/1 [===============================]\tloss: 1118.47\n",
      "Epoch 40/100\n",
      "1/1 [===============================]\tloss: 1073.24\n",
      "Epoch 41/100\n",
      "1/1 [===============================]\tloss: 1031.94\n",
      "Epoch 42/100\n",
      "1/1 [===============================]\tloss: 994.10\n",
      "Epoch 43/100\n",
      "1/1 [===============================]\tloss: 959.42\n",
      "Epoch 44/100\n",
      "1/1 [===============================]\tloss: 927.53\n",
      "Epoch 45/100\n",
      "1/1 [===============================]\tloss: 897.99\n",
      "Epoch 46/100\n",
      "1/1 [===============================]\tloss: 870.52\n",
      "Epoch 47/100\n",
      "1/1 [===============================]\tloss: 844.91\n",
      "Epoch 48/100\n",
      "1/1 [===============================]\tloss: 821.02\n",
      "Epoch 49/100\n",
      "1/1 [===============================]\tloss: 798.75\n",
      "Epoch 50/100\n",
      "1/1 [===============================]\tloss: 777.94\n",
      "Epoch 51/100\n",
      "1/1 [===============================]\tloss: 758.41\n",
      "Epoch 52/100\n",
      "1/1 [===============================]\tloss: 740.04\n",
      "Epoch 53/100\n",
      "1/1 [===============================]\tloss: 722.73\n",
      "Epoch 54/100\n",
      "1/1 [===============================]\tloss: 706.40\n",
      "Epoch 55/100\n",
      "1/1 [===============================]\tloss: 690.95\n",
      "Epoch 56/100\n",
      "1/1 [===============================]\tloss: 676.28\n",
      "Epoch 57/100\n",
      "1/1 [===============================]\tloss: 662.34\n",
      "Epoch 58/100\n",
      "1/1 [===============================]\tloss: 649.06\n",
      "Epoch 59/100\n",
      "1/1 [===============================]\tloss: 636.40\n",
      "Epoch 60/100\n",
      "1/1 [===============================]\tloss: 624.29\n",
      "Epoch 61/100\n",
      "1/1 [===============================]\tloss: 612.72\n",
      "Epoch 62/100\n",
      "1/1 [===============================]\tloss: 601.65\n",
      "Epoch 63/100\n",
      "1/1 [===============================]\tloss: 591.07\n",
      "Epoch 64/100\n",
      "1/1 [===============================]\tloss: 580.91\n",
      "Epoch 65/100\n",
      "1/1 [===============================]\tloss: 571.15\n",
      "Epoch 66/100\n",
      "1/1 [===============================]\tloss: 561.76\n",
      "Epoch 67/100\n",
      "1/1 [===============================]\tloss: 552.73\n",
      "Epoch 68/100\n",
      "1/1 [===============================]\tloss: 544.02\n",
      "Epoch 69/100\n",
      "1/1 [===============================]\tloss: 535.62\n",
      "Epoch 70/100\n",
      "1/1 [===============================]\tloss: 527.51\n",
      "Epoch 71/100\n",
      "1/1 [===============================]\tloss: 519.69\n",
      "Epoch 72/100\n",
      "1/1 [===============================]\tloss: 512.14\n",
      "Epoch 73/100\n",
      "1/1 [===============================]\tloss: 504.85\n",
      "Epoch 74/100\n",
      "1/1 [===============================]\tloss: 497.82\n",
      "Epoch 75/100\n",
      "1/1 [===============================]\tloss: 491.03\n",
      "Epoch 76/100\n",
      "1/1 [===============================]\tloss: 484.48\n",
      "Epoch 77/100\n",
      "1/1 [===============================]\tloss: 478.16\n",
      "Epoch 78/100\n",
      "1/1 [===============================]\tloss: 472.06\n",
      "Epoch 79/100\n",
      "1/1 [===============================]\tloss: 466.18\n",
      "Epoch 80/100\n",
      "1/1 [===============================]\tloss: 460.49\n",
      "Epoch 81/100\n",
      "1/1 [===============================]\tloss: 455.01\n",
      "Epoch 82/100\n",
      "1/1 [===============================]\tloss: 449.72\n",
      "Epoch 83/100\n",
      "1/1 [===============================]\tloss: 444.61\n",
      "Epoch 84/100\n",
      "1/1 [===============================]\tloss: 439.69\n",
      "Epoch 85/100\n",
      "1/1 [===============================]\tloss: 434.92\n",
      "Epoch 86/100\n",
      "1/1 [===============================]\tloss: 430.30\n",
      "Epoch 87/100\n",
      "1/1 [===============================]\tloss: 425.84\n",
      "Epoch 88/100\n",
      "1/1 [===============================]\tloss: 421.51\n",
      "Epoch 89/100\n",
      "1/1 [===============================]\tloss: 417.32\n",
      "Epoch 90/100\n",
      "1/1 [===============================]\tloss: 413.27\n",
      "Epoch 91/100\n",
      "1/1 [===============================]\tloss: 409.34\n",
      "Epoch 92/100\n",
      "1/1 [===============================]\tloss: 405.54\n",
      "Epoch 93/100\n",
      "1/1 [===============================]\tloss: 401.85\n",
      "Epoch 94/100\n",
      "1/1 [===============================]\tloss: 398.27\n",
      "Epoch 95/100\n",
      "1/1 [===============================]\tloss: 394.78\n",
      "Epoch 96/100\n",
      "1/1 [===============================]\tloss: 391.39\n",
      "Epoch 97/100\n",
      "1/1 [===============================]\tloss: 388.10\n",
      "Epoch 98/100\n",
      "1/1 [===============================]\tloss: 384.90\n",
      "Epoch 99/100\n",
      "1/1 [===============================]\tloss: 381.79\n",
      "Epoch 100/100\n",
      "1/1 [===============================]\tloss: 378.76\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "pp.fit(epoches=100,batch=-1);\n",
    "# epoches 总的迭代次数\n",
    "# batch 每次训练的样本数量\n",
    "# batch <= 1 or batch >= 所有样本：训练所有样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54087ba96b7f69cbb53f2c9fca84f0bfbd80e0b1621aa08811937a5cae8ab899"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
